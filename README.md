# ICU Lip-Reading System

A complete production-ready lip-reading system for ICU patient communication using deep learning. The system recognizes 5 critical communication words: **doctor**, **glasses**, **phone**, **pillow**, and **help**.

> **Note**: This repository contains both a complete deep learning system (this README) and a mobile web app implementation. See the mobile app files for the web-based version.

## ğŸ¯ System Overview

This project implements a state-of-the-art lip-reading system specifically designed for ICU environments where patients may be unable to speak due to intubation or other medical conditions.

### Key Features

- **Full LipNet Architecture**: 5.6M parameter model with 3D convolutions and bidirectional GRU
- **Multi-head Attention**: Advanced temporal aggregation for robust sequence classification
- **Production-Ready**: Comprehensive training pipeline with early stopping, checkpointing, and evaluation
- **Class-Balanced Training**: Weighted loss functions to handle imbalanced datasets
- **Comprehensive Evaluation**: Accuracy, F1-scores, confusion matrices, and per-class analysis

## ğŸ—ï¸ Architecture

### Model Components

1. **LipNet Encoder** (4.4M parameters)
   - 3D Convolutional layers: (32, 64, 96) channels
   - Temporal pooling for dimension reduction
   - Bidirectional GRU: 256 hidden units, 2 layers
   - 512-dimensional embeddings

2. **Temporal Aggregation** (1.2M parameters)
   - Multi-head attention mechanism
   - Learnable sequence-level representations

3. **Classification Head**
   - 256 â†’ 128 â†’ 5 classes
   - Dropout regularization

### Target Classes
- **doctor**: Request medical attention
- **glasses**: Need eyewear assistance  
- **phone**: Communication request
- **pillow**: Comfort adjustment
- **help**: General assistance

## ğŸ“Š Performance Results

### Current Performance
- **Test Accuracy**: 40.0% (4/10 samples)
- **Macro F1-Score**: 24.9%
- **Best Validation**: 63.6% accuracy, 46.7% F1-score

### Per-Class Results
| Class   | Precision | Recall | F1-Score |
|---------|-----------|--------|----------|
| GLASSES | 0.667     | 1.000  | 0.800    |
| DOCTOR  | 0.333     | 0.667  | 0.444    |
| PHONE   | 0.000     | 0.000  | 0.000    |
| PILLOW  | 0.000     | 0.000  | 0.000    |
| HELP    | 0.000     | 0.000  | 0.000    |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/jasonhall1985/LRP-11.9.25.git
cd LRP-11.9.25

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch torchvision torchaudio
pip install opencv-python scikit-learn pyyaml tqdm
pip install numpy matplotlib seaborn
```

### Training

```bash
# Train the full system
python src/train_full_system.py --config configs/icu_classifier.yaml

# Train with custom configuration
python src/train_full_system.py --config your_config.yaml
```

### Data Preprocessing

```bash
# Preprocess video data
python src/simple_preprocess.py

# Check preprocessing results
python src/datasets.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lipnet_encoder.py      # LipNet encoder implementation
â”‚   â”‚   â””â”€â”€ icu_classifier.py      # Complete classifier
â”‚   â”œâ”€â”€ datasets.py                # Data loading and preprocessing
â”‚   â”œâ”€â”€ simple_preprocess.py       # Video preprocessing pipeline
â”‚   â”œâ”€â”€ train_full_system.py       # Main training script
â”‚   â”œâ”€â”€ train_classifier.py        # Alternative training script
â”‚   â””â”€â”€ grid_dataset.py           # GRID dataset utilities
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ icu_classifier.yaml        # Main training configuration
â”‚   â””â”€â”€ lipnet_grid.yaml          # GRID pretraining config
â”œâ”€â”€ simple_processed_data/         # Preprocessed video data
â”œâ”€â”€ models/                        # Saved model checkpoints
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

### Training Parameters

Key configuration options in `configs/icu_classifier.yaml`:

```yaml
# Model Architecture
model:
  encoder:
    embedding_dim: 512
    conv_channels: [32, 64, 96]
    gru_hidden_size: 256
    gru_num_layers: 2

# Training Settings
training:
  epochs: 50
  learning_rate: 0.001
  batch_size: 16
  early_stopping:
    patience: 10

# Performance Targets
targets:
  min_accuracy: 0.80
  min_macro_f1: 0.80
```

## ğŸ”¬ Technical Details

### Data Processing
- **Frame Extraction**: 16 frames per video sequence
- **Resolution**: 96x96 grayscale images
- **Preprocessing**: Center cropping with OpenCV fallback
- **Augmentation**: Temporal and spatial augmentations during training

### Training Strategy
- **Loss Function**: Cross-entropy with class balancing
- **Optimizer**: AdamW with weight decay
- **Scheduler**: Step LR with gamma=0.5
- **Regularization**: Dropout, gradient clipping, early stopping

## ğŸ“ˆ Future Improvements

To reach the target 80% accuracy and F1-score:

1. **Data Augmentation**: Expand training data through advanced augmentations
2. **Transfer Learning**: Use pretrained visual features
3. **Ensemble Methods**: Combine multiple models
4. **Architecture Optimization**: Hyperparameter tuning and model scaling
5. **Data Collection**: Expand dataset with more diverse samples

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- LipNet architecture based on "LipNet: End-to-End Sentence-level Lipreading"
- GRID corpus for speech recognition research
- PyTorch deep learning framework

## ğŸ“ Contact

For questions or collaboration opportunities, please open an issue or contact the repository owner.

---

**Status**: âœ… Complete implementation with full training pipeline
**Performance**: 40% accuracy, 24.9% macro-F1 (baseline established)
**Next Steps**: Data augmentation and hyperparameter optimization for target performance

## ğŸ“± Mobile Web App Version

This repository also contains a mobile-friendly web application version of the lip-reading system. The mobile app features:

- **Real-time lip detection** using MediaPipe Face Mesh
- **5-word vocabulary recognition**: doctor, glasses, help, pillow, phone
- **Mobile-responsive design** optimized for iPhone browsers
- **Live webcam integration** for real-time predictions
- **TensorFlow/Keras CNN + LSTM** architecture

The mobile app was developed for educational purposes and provides a lightweight alternative to the full deep learning system documented above.
