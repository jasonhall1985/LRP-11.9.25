#!/usr/bin/env python3
"""
Phase 2: Enhanced Training Pipeline for Lip-Reading Classifier
Implementing comprehensive improvements to achieve 60-75% accuracy.
"""

import os
import sys
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import cv2
import json
import time
import math
from datetime import datetime
from pathlib import Path
import logging
import psutil
from sklearn.metrics import accuracy_score, f1_score, classification_report

def get_memory_usage():
    """Get current memory usage in MB."""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def set_random_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class EnhancedLipDataset(Dataset):
    """Enhanced dataset with standardized preprocessing and strategic augmentations."""
    
    def __init__(self, video_paths, labels, split='train', augment=True):
        self.video_paths = video_paths
        self.labels = labels
        self.split = split
        self.augment = augment and (split == 'train')
        
        self.class_to_idx = {'doctor': 0, 'glasses': 1, 'help': 2, 'phone': 3, 'pillow': 4}
        print(f"üìä {split.upper()} Dataset: {len(self.video_paths)} videos")
        
    def __len__(self):
        return len(self.video_paths)
    
    def load_video_standardized(self, video_path):
        """Load video with standardized preprocessing pipeline."""
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"Cannot open video: {video_path}")
            
            frames = []
            while len(frames) < 40:  # Load more frames for temporal sampling
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Convert to grayscale
                if len(frame.shape) == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # Standardized mouth ROI: 224x224 ‚Üí 112x112 center crop
                h, w = frame.shape
                if h != 224 or w != 224:
                    frame = cv2.resize(frame, (224, 224))
                
                # Center crop to 112x112 (mouth region)
                start_h, start_w = (224 - 112) // 2, (224 - 112) // 2
                frame = frame[start_h:start_h + 112, start_w:start_w + 112]
                
                frames.append(frame)
            
            cap.release()
            
            # Temporal standardization: uniform sampling to 32 frames
            if len(frames) >= 32:
                indices = np.linspace(0, len(frames) - 1, 32, dtype=int)
                frames = [frames[i] for i in indices]
            else:
                # Pad with last frame
                while len(frames) < 32:
                    frames.append(frames[-1] if frames else np.zeros((112, 112), dtype=np.uint8))
            
            return np.array(frames[:32])
            
        except Exception as e:
            print(f"‚ùå Error loading video {video_path}: {str(e)}")
            return np.zeros((32, 112, 112), dtype=np.uint8)
    
    def apply_strategic_augmentations(self, frames):
        """Apply strategic training augmentations."""
        if not self.augment:
            return frames
        
        # Temporal augmentations
        if random.random() < 0.3:
            # ¬±2 frame jitter
            jitter = random.randint(-2, 2)
            if jitter != 0:
                if jitter > 0:
                    frames = frames[jitter:]
                    frames = np.concatenate([frames, np.repeat(frames[-1:], jitter, axis=0)])
                else:
                    frames = frames[:jitter]
                    frames = np.concatenate([np.repeat(frames[:1], -jitter, axis=0), frames])
        
        if random.random() < 0.1:
            # 10% random frame dropout
            mask = np.random.random(32) > 0.1
            if mask.sum() > 16:  # Keep at least half the frames
                frames = frames[mask]
                # Pad back to 32 frames
                while len(frames) < 32:
                    frames = np.concatenate([frames, frames[-1:]])
        
        # Spatial augmentations
        if random.random() < 0.5:
            # ¬±3px translation
            tx = random.randint(-3, 3)
            ty = random.randint(-3, 3)
            if tx != 0 or ty != 0:
                M = np.float32([[1, 0, tx], [0, 1, ty]])
                for i in range(len(frames)):
                    frames[i] = cv2.warpAffine(frames[i], M, (112, 112))
        
        if random.random() < 0.3:
            # ¬±5% scale
            scale = random.uniform(0.95, 1.05)
            if scale != 1.0:
                new_size = int(112 * scale)
                for i in range(len(frames)):
                    resized = cv2.resize(frames[i], (new_size, new_size))
                    if scale > 1.0:
                        # Crop center
                        start = (new_size - 112) // 2
                        frames[i] = resized[start:start+112, start:start+112]
                    else:
                        # Pad
                        pad = (112 - new_size) // 2
                        frames[i] = cv2.copyMakeBorder(resized, pad, 112-new_size-pad, 
                                                     pad, 112-new_size-pad, cv2.BORDER_REFLECT)
        
        if random.random() < 0.3:
            # ¬±3¬∞ rotation
            angle = random.uniform(-3, 3)
            if abs(angle) > 0.1:
                center = (56, 56)
                M = cv2.getRotationMatrix2D(center, angle, 1.0)
                for i in range(len(frames)):
                    frames[i] = cv2.warpAffine(frames[i], M, (112, 112))
        
        # Appearance augmentations
        if random.random() < 0.3:
            # ¬±10% brightness/contrast
            brightness = random.uniform(0.9, 1.1)
            contrast = random.uniform(0.9, 1.1)
            frames = np.clip(frames * contrast + (brightness - 1) * 128, 0, 255).astype(np.uint8)
        
        # Cutout (avoid lip center region)
        if random.random() < 0.2:
            # Small 8x8 patches outside center 32x32 region
            for _ in range(2):
                x = random.randint(0, 104)
                y = random.randint(0, 104)
                # Skip if in center lip region
                if 40 <= x <= 72 and 40 <= y <= 72:
                    continue
                frames[:, y:y+8, x:x+8] = 0
        
        return frames
    
    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]
        
        # Load video with standardized preprocessing
        frames = self.load_video_standardized(video_path)
        
        # Apply strategic augmentations
        frames = self.apply_strategic_augmentations(frames)
        
        # Standardized normalization: [0,1] with mean=0.5, std=0.5
        frames = frames.astype(np.float32) / 255.0
        frames = (frames - 0.5) / 0.5  # Normalize to [-1, 1]
        
        # Convert to tensor: (C, T, H, W) format
        frames = torch.from_numpy(frames).unsqueeze(0)
        
        return frames, label

class EnhancedLipModel(nn.Module):
    """Enhanced model with BiGRU head to address underfitting."""
    
    def __init__(self, num_classes=5):
        super(EnhancedLipModel, self).__init__()
        
        # 3D CNN Backbone (keep current architecture)
        self.backbone = nn.Sequential(
            # First conv block
            nn.Conv3d(1, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)),
            
            # Second conv block
            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),
            
            # Third conv block
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1)),
            nn.BatchNorm3d(128),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool3d((4, 1, 1)),  # Keep temporal dimension for GRU
        )
        
        # Enhanced Head with BiGRU
        self.temporal_pool = nn.AdaptiveAvgPool3d((4, 1, 1))  # Reduce to 4 time steps
        
        # 2-layer Bidirectional GRU
        self.gru = nn.GRU(
            input_size=128,
            hidden_size=256,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        
        # Enhanced classifier head
        self.classifier = nn.Sequential(
            nn.LayerNorm(512),  # 256 * 2 (bidirectional)
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize model weights."""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.GRU):
                for name, param in m.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
    
    def forward(self, x):
        # x shape: (batch, 1, 32, 112, 112)
        
        # 3D CNN backbone
        features = self.backbone(x)  # (batch, 128, 4, 1, 1)
        
        # Prepare for GRU: (batch, seq_len, features)
        batch_size = features.size(0)
        features = features.view(batch_size, 128, 4).transpose(1, 2)  # (batch, 4, 128)
        
        # BiGRU processing
        gru_out, _ = self.gru(features)  # (batch, 4, 512)
        
        # Use last time step output
        final_features = gru_out[:, -1, :]  # (batch, 512)
        
        # Classification
        output = self.classifier(final_features)
        
        return output

class LabelSmoothingCrossEntropy(nn.Module):
    """Label smoothing cross entropy loss."""
    
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        confidence = 1. - self.smoothing
        log_probs = F.log_softmax(pred, dim=-1)
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -log_probs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()

class CosineWarmupScheduler:
    """Cosine annealing with warmup scheduler."""
    
    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-5):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.min_lr = min_lr
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Warmup phase
            lr_scale = (epoch + 1) / self.warmup_epochs
            for i, param_group in enumerate(self.optimizer.param_groups):
                param_group['lr'] = self.base_lrs[i] * lr_scale
        else:
            # Cosine annealing phase
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            for i, param_group in enumerate(self.optimizer.param_groups):
                param_group['lr'] = self.min_lr + (self.base_lrs[i] - self.min_lr) * \
                                  0.5 * (1 + math.cos(math.pi * progress))

class EMAModel:
    """Exponential Moving Average model."""
    
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        # Initialize shadow weights
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

def verify_speaker_disjoint_splits(train_videos, val_videos, test_videos):
    """Verify that data splits are speaker-disjoint."""
    print("üîç Verifying speaker-disjoint splits...")
    
    def extract_speaker_id(video_path):
        # Extract speaker ID from filename (assuming format: class_speaker_id.mp4)
        filename = Path(video_path).stem
        parts = filename.split('_')
        if len(parts) >= 2:
            return f"{parts[0]}_{parts[1]}"  # class_speaker
        return filename
    
    train_speakers = set(extract_speaker_id(v) for v in train_videos)
    val_speakers = set(extract_speaker_id(v) for v in val_videos)
    test_speakers = set(extract_speaker_id(v) for v in test_videos)
    
    # Check for overlaps
    train_val_overlap = train_speakers & val_speakers
    train_test_overlap = train_speakers & test_speakers
    val_test_overlap = val_speakers & test_speakers
    
    print(f"üìä Speaker analysis:")
    print(f"   ‚Ä¢ Train speakers: {len(train_speakers)}")
    print(f"   ‚Ä¢ Val speakers: {len(val_speakers)}")
    print(f"   ‚Ä¢ Test speakers: {len(test_speakers)}")
    
    if train_val_overlap or train_test_overlap or val_test_overlap:
        print(f"‚ö†Ô∏è  WARNING: Speaker overlap detected!")
        if train_val_overlap:
            print(f"   ‚Ä¢ Train-Val overlap: {train_val_overlap}")
        if train_test_overlap:
            print(f"   ‚Ä¢ Train-Test overlap: {train_test_overlap}")
        if val_test_overlap:
            print(f"   ‚Ä¢ Val-Test overlap: {val_test_overlap}")
        return False
    else:
        print("‚úÖ Splits are speaker-disjoint!")
        return True

def create_speaker_disjoint_splits(dataset_path="corrected_balanced_dataset"):
    """Create speaker-disjoint train/validation/test splits."""
    print("üìä Creating speaker-disjoint data splits...")
    
    video_files = list(Path(dataset_path).glob("*.mp4"))
    if len(video_files) == 0:
        raise ValueError(f"No video files found in {dataset_path}")
    
    print(f"Found {len(video_files)} videos")
    
    # Group videos by class and speaker
    class_speaker_videos = {}
    class_to_idx = {'doctor': 0, 'glasses': 1, 'help': 2, 'phone': 3, 'pillow': 4}
    
    for video_file in video_files:
        filename = video_file.stem
        parts = filename.split('_')
        class_name = parts[0]
        
        if class_name in class_to_idx:
            speaker_id = f"{parts[0]}_{parts[1]}" if len(parts) >= 2 else filename
            
            if class_name not in class_speaker_videos:
                class_speaker_videos[class_name] = {}
            if speaker_id not in class_speaker_videos[class_name]:
                class_speaker_videos[class_name][speaker_id] = []
            
            class_speaker_videos[class_name][speaker_id].append(str(video_file))
    
    # Create speaker-disjoint splits
    train_videos, train_labels = [], []
    val_videos, val_labels = [], []
    test_videos, test_labels = [], []
    
    for class_name, speakers in class_speaker_videos.items():
        class_idx = class_to_idx[class_name]
        speaker_list = list(speakers.keys())
        random.shuffle(speaker_list)
        
        print(f"üìä {class_name}: {len(speaker_list)} speakers, {sum(len(videos) for videos in speakers.values())} videos")
        
        # Distribute speakers across splits
        for i, speaker_id in enumerate(speaker_list):
            videos = speakers[speaker_id]
            
            if i % 10 < 8:  # 80% for training
                train_videos.extend(videos)
                train_labels.extend([class_idx] * len(videos))
            elif i % 10 == 8:  # 10% for validation
                val_videos.extend(videos)
                val_labels.extend([class_idx] * len(videos))
            else:  # 10% for test
                test_videos.extend(videos)
                test_labels.extend([class_idx] * len(videos))
    
    print(f"üìä Final splits: Train={len(train_videos)}, Val={len(val_videos)}, Test={len(test_videos)}")
    
    # Verify speaker disjoint
    verify_speaker_disjoint_splits(train_videos, val_videos, test_videos)
    
    return (train_videos, train_labels), (val_videos, val_labels), (test_videos, test_labels)

class EnhancedTrainer:
    """Enhanced training manager with staged fine-tuning and comprehensive monitoring."""

    def __init__(self, model, train_loader, val_loader, test_loader, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = device

        # Enhanced training configuration
        self.criterion = LabelSmoothingCrossEntropy(smoothing=0.1)

        # Staged learning rates
        self.head_params = []
        self.backbone_params = []

        # Separate parameters for different learning rates
        for name, param in model.named_parameters():
            if 'gru' in name or 'classifier' in name:
                self.head_params.append(param)
            else:
                self.backbone_params.append(param)

        # AdamW optimizer with different learning rates
        self.optimizer = optim.AdamW([
            {'params': self.head_params, 'lr': 6e-4, 'weight_decay': 1e-4},
            {'params': self.backbone_params, 'lr': 1e-4, 'weight_decay': 1e-4}
        ])

        # Cosine scheduler with warmup
        self.scheduler = CosineWarmupScheduler(
            self.optimizer, warmup_epochs=1, total_epochs=8, min_lr=1e-5
        )

        # EMA model
        self.ema = EMAModel(model, decay=0.999)

        # Training state
        self.epoch = 0
        self.best_val_f1 = 0.0
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_accuracies = []
        self.val_f1_scores = []
        self.stage = 1

        # Setup logging
        self.setup_logging()

    def setup_logging(self):
        """Setup comprehensive logging."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_dir = f"enhanced_training_{timestamp}"
        os.makedirs(self.experiment_dir, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{self.experiment_dir}/training.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"üöÄ Enhanced training started: {self.experiment_dir}")

    def freeze_backbone(self):
        """Freeze backbone parameters."""
        for param in self.backbone_params:
            param.requires_grad = False
        self.logger.info("üîí Backbone frozen")

    def unfreeze_backbone(self):
        """Unfreeze backbone parameters."""
        for param in self.backbone_params:
            param.requires_grad = True
        self.logger.info("üîì Backbone unfrozen")

    def unfreeze_last_block(self):
        """Unfreeze only the last backbone block."""
        # Unfreeze last conv block
        for name, param in self.model.named_parameters():
            if 'backbone.6' in name or 'backbone.7' in name or 'backbone.8' in name:
                param.requires_grad = True
        self.logger.info("üîì Last backbone block unfrozen")

    def update_learning_rates(self, head_lr, backbone_lr):
        """Update learning rates for staged training."""
        self.optimizer.param_groups[0]['lr'] = head_lr  # Head
        self.optimizer.param_groups[1]['lr'] = backbone_lr  # Backbone
        self.logger.info(f"üìö Learning rates updated: Head={head_lr:.2e}, Backbone={backbone_lr:.2e}")

    def train_epoch(self):
        """Train for one epoch with comprehensive monitoring."""
        self.model.train()

        # Set BatchNorm to eval mode if batch size < 8
        if self.train_loader.batch_size < 8:
            for module in self.model.modules():
                if isinstance(module, nn.BatchNorm3d):
                    module.eval()

        total_loss = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []

        for batch_idx, (data, target) in enumerate(self.train_loader):
            # Memory monitoring
            if batch_idx % 10 == 0:
                memory_mb = get_memory_usage()
                if memory_mb > 2000:
                    import gc
                    gc.collect()
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None

            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            # Update EMA
            self.ema.update()

            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)

            all_preds.extend(pred.cpu().numpy().flatten())
            all_targets.extend(target.cpu().numpy())

            if batch_idx % 10 == 0:
                memory_mb = get_memory_usage()
                self.logger.info(f"Stage {self.stage}, Epoch {self.epoch}, Batch {batch_idx}/{len(self.train_loader)}, "
                               f"Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%, "
                               f"Memory: {memory_mb:.1f}MB")

        avg_loss = total_loss / len(self.train_loader)
        train_acc = 100. * correct / total
        train_f1 = f1_score(all_targets, all_preds, average='macro') * 100

        self.train_losses.append(avg_loss)
        return avg_loss, train_acc, train_f1

    def validate(self, use_ema=False):
        """Validate the model with optional EMA."""
        if use_ema:
            self.ema.apply_shadow()

        self.model.eval()
        correct = 0
        total = 0
        all_preds = []
        all_targets = []

        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)

                # Test Time Augmentation (TTA)
                outputs = []

                # Original
                output = self.model(data)
                outputs.append(output)

                # Temporal crops (start, middle, end)
                if data.size(2) >= 32:  # If we have enough frames
                    # Start crop
                    start_data = data[:, :, :28, :, :]  # First 28 frames
                    start_data = F.pad(start_data, (0, 0, 0, 0, 0, 4))  # Pad to 32
                    outputs.append(self.model(start_data))

                    # End crop
                    end_data = data[:, :, -28:, :, :]  # Last 28 frames
                    end_data = F.pad(end_data, (0, 0, 0, 0, 4, 0))  # Pad to 32
                    outputs.append(self.model(end_data))

                # Average TTA predictions
                output = torch.stack(outputs).mean(0)

                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())

        if use_ema:
            self.ema.restore()

        val_acc = 100. * correct / total
        val_f1 = f1_score(all_targets, all_preds, average='macro') * 100

        self.val_accuracies.append(val_acc)
        self.val_f1_scores.append(val_f1)

        return val_acc, val_f1, all_preds, all_targets

    def staged_training(self):
        """Implement staged fine-tuning strategy."""
        self.logger.info("üéØ Starting staged fine-tuning...")

        patience_counter = 0
        max_patience = 4

        # Stage 1: Freeze backbone, train head only (1 epoch)
        self.stage = 1
        self.logger.info("üî• Stage 1: Training head only")
        self.freeze_backbone()
        self.update_learning_rates(head_lr=6e-4, backbone_lr=0)

        for epoch in range(1):
            self.epoch = epoch + 1
            self.scheduler.step(self.epoch - 1)

            train_loss, train_acc, train_f1 = self.train_epoch()
            val_acc, val_f1, val_preds, val_targets = self.validate()
            ema_val_acc, ema_val_f1, _, _ = self.validate(use_ema=True)

            self.logger.info(f"Stage 1, Epoch {self.epoch} - "
                           f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.2f}%, "
                           f"Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.2f}%, "
                           f"EMA Val Acc: {ema_val_acc:.2f}%, EMA Val F1: {ema_val_f1:.2f}%")

        # Stage 2: Unfreeze last block (3 epochs)
        self.stage = 2
        self.logger.info("üî• Stage 2: Training head + last backbone block")
        self.unfreeze_last_block()
        self.update_learning_rates(head_lr=4e-4, backbone_lr=1e-4)

        for epoch in range(3):
            self.epoch = epoch + 2  # Continue epoch counting
            self.scheduler.step(self.epoch - 1)

            train_loss, train_acc, train_f1 = self.train_epoch()
            val_acc, val_f1, val_preds, val_targets = self.validate()
            ema_val_acc, ema_val_f1, _, _ = self.validate(use_ema=True)

            # Check for improvement
            is_best = val_f1 > self.best_val_f1
            if is_best:
                self.best_val_f1 = val_f1
                self.best_val_acc = val_acc
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), f"{self.experiment_dir}/best_model.pth")
                torch.save(self.ema.shadow, f"{self.experiment_dir}/best_ema_model.pth")
            else:
                patience_counter += 1

            self.logger.info(f"Stage 2, Epoch {self.epoch} - "
                           f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.2f}%, "
                           f"Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.2f}%, "
                           f"EMA Val Acc: {ema_val_acc:.2f}%, EMA Val F1: {ema_val_f1:.2f}%, "
                           f"Best F1: {self.best_val_f1:.2f}%")

            if patience_counter >= max_patience:
                self.logger.info(f"‚èπÔ∏è  Early stopping in Stage 2")
                break

        # Stage 3: Unfreeze all layers (4 epochs)
        if patience_counter < max_patience:
            self.stage = 3
            self.logger.info("üî• Stage 3: Training all layers")
            self.unfreeze_backbone()
            self.update_learning_rates(head_lr=2e-4, backbone_lr=7e-5)

            for epoch in range(4):
                self.epoch = epoch + 5  # Continue epoch counting
                self.scheduler.step(self.epoch - 1)

                train_loss, train_acc, train_f1 = self.train_epoch()
                val_acc, val_f1, val_preds, val_targets = self.validate()
                ema_val_acc, ema_val_f1, _, _ = self.validate(use_ema=True)

                # Check for improvement
                is_best = val_f1 > self.best_val_f1
                if is_best:
                    self.best_val_f1 = val_f1
                    self.best_val_acc = val_acc
                    patience_counter = 0
                    torch.save(self.model.state_dict(), f"{self.experiment_dir}/best_model.pth")
                    torch.save(self.ema.shadow, f"{self.experiment_dir}/best_ema_model.pth")
                else:
                    patience_counter += 1

                self.logger.info(f"Stage 3, Epoch {self.epoch} - "
                               f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.2f}%, "
                               f"Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.2f}%, "
                               f"EMA Val Acc: {ema_val_acc:.2f}%, EMA Val F1: {ema_val_f1:.2f}%, "
                               f"Best F1: {self.best_val_f1:.2f}%")

                if patience_counter >= max_patience:
                    self.logger.info(f"‚èπÔ∏è  Early stopping in Stage 3")
                    break

        # Final test evaluation
        return self.final_test_evaluation()

    def final_test_evaluation(self):
        """Comprehensive final test evaluation."""
        self.logger.info("üîç Final test evaluation...")

        # Load best model
        best_model_path = f"{self.experiment_dir}/best_model.pth"
        if os.path.exists(best_model_path):
            self.model.load_state_dict(torch.load(best_model_path, map_location=self.device))

        # Test with regular model
        self.model.eval()
        correct = 0
        total = 0
        all_preds = []
        all_targets = []

        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)

                # Enhanced TTA for final evaluation
                outputs = []

                # Original
                outputs.append(self.model(data))

                # Temporal variations
                if data.size(2) >= 32:
                    # Multiple temporal crops
                    for start_idx in [0, 2, 4]:
                        if start_idx + 28 <= data.size(2):
                            crop_data = data[:, :, start_idx:start_idx+28, :, :]
                            crop_data = F.pad(crop_data, (0, 0, 0, 0, 0, 4))
                            outputs.append(self.model(crop_data))

                # Spatial jitter (¬±2px)
                for dx, dy in [(0, 2), (2, 0), (0, -2), (-2, 0)]:
                    if abs(dx) <= 2 and abs(dy) <= 2:
                        shifted_data = torch.roll(data, shifts=(dx, dy), dims=(3, 4))
                        outputs.append(self.model(shifted_data))

                # Average all predictions
                output = torch.stack(outputs).mean(0)

                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

                all_preds.extend(pred.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy())

        test_acc = 100. * correct / total
        test_f1 = f1_score(all_targets, all_preds, average='macro') * 100

        # Test with EMA model
        best_ema_path = f"{self.experiment_dir}/best_ema_model.pth"
        if os.path.exists(best_ema_path):
            # Load EMA weights
            ema_weights = torch.load(best_ema_path, map_location=self.device)
            original_weights = {}
            for name, param in self.model.named_parameters():
                if name in ema_weights:
                    original_weights[name] = param.data.clone()
                    param.data = ema_weights[name]

            # Test EMA model
            ema_correct = 0
            ema_all_preds = []

            with torch.no_grad():
                for data, target in self.test_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    pred = output.argmax(dim=1, keepdim=True)
                    ema_correct += pred.eq(target.view_as(pred)).sum().item()
                    ema_all_preds.extend(pred.cpu().numpy().flatten())

            ema_test_acc = 100. * ema_correct / total
            ema_test_f1 = f1_score(all_targets, ema_all_preds, average='macro') * 100

            # Restore original weights
            for name, param in self.model.named_parameters():
                if name in original_weights:
                    param.data = original_weights[name]
        else:
            ema_test_acc = test_acc
            ema_test_f1 = test_f1

        # Generate detailed classification report
        class_names = ['doctor', 'glasses', 'help', 'phone', 'pillow']
        report = classification_report(all_targets, all_preds, target_names=class_names)

        self.logger.info(f"üéØ FINAL RESULTS:")
        self.logger.info(f"   ‚Ä¢ Regular Model - Test Acc: {test_acc:.2f}%, Test F1: {test_f1:.2f}%")
        self.logger.info(f"   ‚Ä¢ EMA Model - Test Acc: {ema_test_acc:.2f}%, Test F1: {ema_test_f1:.2f}%")
        self.logger.info(f"   ‚Ä¢ Best Val Acc: {self.best_val_acc:.2f}%, Best Val F1: {self.best_val_f1:.2f}%")
        self.logger.info(f"üìä Classification Report:\n{report}")

        # Save comprehensive results
        results = {
            'test_accuracy': test_acc,
            'test_f1_score': test_f1,
            'ema_test_accuracy': ema_test_acc,
            'ema_test_f1_score': ema_test_f1,
            'best_val_accuracy': self.best_val_acc,
            'best_val_f1_score': self.best_val_f1,
            'total_epochs': self.epoch,
            'classification_report': report,
            'final_stage': self.stage
        }

        with open(f"{self.experiment_dir}/final_results.json", 'w') as f:
            json.dump(results, f, indent=2)

        return max(test_acc, ema_test_acc)

def main():
    """Main enhanced training execution."""
    print("üöÄ ENHANCED LIP-READING TRAINING - TARGET: 60-75% ACCURACY")
    print("=" * 80)
    print(f"üíæ Initial memory: {get_memory_usage():.1f} MB")

    # Set random seeds
    set_random_seeds(42)

    # Device
    device = torch.device('cpu')
    print(f"üñ•Ô∏è  Device: {device}")

    # Create speaker-disjoint data splits
    (train_videos, train_labels), (val_videos, val_labels), (test_videos, test_labels) = create_speaker_disjoint_splits()

    # Create enhanced datasets
    train_dataset = EnhancedLipDataset(train_videos, train_labels, split='train', augment=True)
    val_dataset = EnhancedLipDataset(val_videos, val_labels, split='val', augment=False)
    test_dataset = EnhancedLipDataset(test_videos, test_labels, split='test', augment=False)

    # Create data loaders (batch_size=1 for memory efficiency)
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

    print(f"üíæ After data loading: {get_memory_usage():.1f} MB")

    # Create enhanced model
    model = EnhancedLipModel(num_classes=5).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"üß† Enhanced Model:")
    print(f"   ‚Ä¢ Total parameters: {total_params:,}")
    print(f"   ‚Ä¢ Trainable parameters: {trainable_params:,}")
    print(f"   ‚Ä¢ Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    print(f"üíæ After model creation: {get_memory_usage():.1f} MB")

    # Create enhanced trainer
    trainer = EnhancedTrainer(model, train_loader, val_loader, test_loader, device)

    print(f"üíæ After trainer setup: {get_memory_usage():.1f} MB")

    # Start staged training
    print("\nüéØ STARTING ENHANCED TRAINING WITH STAGED FINE-TUNING")
    print("=" * 60)

    start_time = time.time()
    final_accuracy = trainer.staged_training()
    training_time = (time.time() - start_time) / 60

    print(f"\nüéâ ENHANCED TRAINING COMPLETED!")
    print("=" * 50)
    print(f"üéØ Final Test Accuracy: {final_accuracy:.2f}%")
    print(f"‚è±Ô∏è  Training Time: {training_time:.1f} minutes")
    print(f"üíæ Final Memory Usage: {get_memory_usage():.1f} MB")
    print(f"üìÅ Results Directory: {trainer.experiment_dir}")

    # Success evaluation
    if final_accuracy >= 60:
        print(f"‚úÖ SUCCESS: Target accuracy (60%+) achieved! üéâ")
        if final_accuracy >= 75:
            print(f"üèÜ EXCELLENT: Exceeded expectations (75%+)!")
        elif final_accuracy >= 70:
            print(f"üåü GREAT: Strong performance (70%+)!")
    elif final_accuracy >= 50:
        print(f"üìà GOOD PROGRESS: Significant improvement from 40% baseline")
    else:
        print(f"‚ö†Ô∏è  NEEDS IMPROVEMENT: Consider fallback options")

    print(f"\nüìä IMPROVEMENT SUMMARY:")
    print(f"   ‚Ä¢ Baseline (previous): 40.00%")
    print(f"   ‚Ä¢ Enhanced (current): {final_accuracy:.2f}%")
    print(f"   ‚Ä¢ Improvement: +{final_accuracy - 40:.2f} percentage points")
    print(f"   ‚Ä¢ Relative improvement: {((final_accuracy / 40) - 1) * 100:.1f}%")

    return final_accuracy

if __name__ == "__main__":
    try:
        final_accuracy = main()

        # Update task status based on results
        if final_accuracy >= 60:
            print(f"\nüéØ MISSION ACCOMPLISHED: Enhanced training achieved target accuracy!")
        else:
            print(f"\nüîÑ READY FOR NEXT ITERATION: Consider fallback improvements")

    except Exception as e:
        print(f"\nüí• ERROR: {e}")
        import traceback
        traceback.print_exc()
